---
title: "ML time"
output: html_document
date: '2022-06-25'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## loading the datasets
```{r}
library(readr)
library(dplyr)
Senwave <- read_csv("../training_sets/SenWave/labeledtweets/labeledtweets/labeledEn.csv")
US <- read_csv("../training_sets/1000labels.csv")
wo_US <- read_csv("../training_sets/sample_wo_US.csv")
wo_US_2 <- read_csv("../training_sets/sample_wo_US_2.csv")
```
### Merge all of the data together
we want to try 3 different sets
1. All 5,000 
2. SenWave plus non-US sample
3. 2,000 hand labeled
```{r}
Senwave <- Senwave %>%
  filter(is.na(My_Fear)==FALSE) %>%
  select(Tweet, My_Fear, My_Denial)

colnames(Senwave) <- c("text", "Pro", "Anti")

US$Denial <- ifelse(US$`Anti lockdown` == 1, 1, US$Denial)
US$Fear <- ifelse(US$`Pro lockdown` == 1, 1, US$Fear)

US <- US %>%
  select(text, Denial, Fear)

colnames(US) <- c("text", "Pro", "Anti")

wo_US <- wo_US %>%
  select(text, New_Fear, New_Denial)

colnames(wo_US) <- c("text", "Pro", "Anti")

wo_US_2 <- wo_US_2 %>%
  select(text, Pro, Anti)


train_2000 <- rbind(wo_US, wo_US_2)
train_4000 <- rbind(Senwave, wo_US, wo_US_2)
train_5000 <- rbind(Senwave, US, wo_US, wo_US_2)


train_2000$class <- ifelse(train_2000$Pro == 1, 1, 0)
train_2000$class <- ifelse(train_2000$Anti ==1, 2, train_2000$class)

train_2000 <- train_2000 %>%
  filter(is.na(class)==FALSE)

train_2000

train_4000$class <- ifelse(train_4000$Pro == 1, 1, 0)
train_4000$class <- ifelse(train_4000$Anti ==1, 2, train_4000$class)

train_4000 <- train_4000 %>%
  filter(is.na(class)==FALSE)

train_4000

train_5000$class <- ifelse(train_5000$Pro == 1, 1, 0)
train_5000$class <- ifelse(train_5000$Anti ==1, 2, train_5000$class)

train_5000 <- train_5000 %>%
  filter(is.na(class)==FALSE)

train_5000



```



## Transforming to a dfm
I removed all punctuation, numbers, and symbols, any token with fewer than two characters; and terms which had fewer than five total occurrences throughout the corpus or were contained in fewer than two total speeches.
Central to the process of transforming texts into a DFM is the “bag-of-words” assumption which ignores word order focusing instead on word frequency (Manning, Raghavan, and Schutze 2008).
```{r}
library(quanteda)
library(e1071)

train_2000$class <- factor(train_2000$class)
corpus <- corpus(train_2000, text_field="text") # create a corpus
## I dont want to remove symbols as they are often quite important in tweets
toks <- tokens(corpus, remove_punct = TRUE, remove_url=TRUE, remove_numbers = TRUE, verbose=TRUE)
## making 3 different dfms and using cross validation to decide which to use
toks_stop <- tokens_remove(toks, c(
  stopwords("english"),"https", "rt", "http", "u", "amp"))

toks_ngram <- tokens_ngrams(toks_stop, n = 1:2)
bigram_2_2000 <- dfm(toks_ngram, tolower=TRUE)
bigram_2_2000 <- dfm_trim(bigram_2_2000, min_docfreq = 2)

bigram_2_2000
# 5,388 features

toks_ngram <- tokens_ngrams(toks_stop, n = 1:3)
trigram_2_2000 <- dfm(toks_ngram, tolower=TRUE)
trigram_2_2000 <- dfm_trim(trigram_2_2000, min_docfreq = 2)

trigram_2_2000
# 6,125 features
```

```{r}
train_4000$class <- factor(train_4000$class)
corpus <- corpus(train_4000, text_field="text") # create a corpus
## I dont want to remove symbols as they are often quite important in tweets
toks <- tokens(corpus, remove_punct = TRUE, remove_url=TRUE, remove_numbers = TRUE, verbose=TRUE)
## making 3 different dfms and using cross validation to decide which to use
toks_stop <- tokens_remove(toks, c(
  stopwords("english"),"https", "rt", "http", "u", "amp"))

toks_ngram <- tokens_ngrams(toks_stop, n = 1:2)
bigram_2_4000 <- dfm(toks_ngram, tolower=TRUE)
bigram_2_4000 <- dfm_trim(bigram_2_4000, min_docfreq = 2)

bigram_2_4000
# 8,137 features

toks_ngram <- tokens_ngrams(toks_stop, n = 1:3)
trigram_2_4000 <- dfm(toks_ngram, tolower=TRUE)
trigram_2_4000 <- dfm_trim(trigram_2_4000, min_docfreq = 2)

trigram_2_4000
# 9,355 features
```

```{r}
train_5000$class <- factor(train_5000$class)
corpus <- corpus(train_5000, text_field="text") # create a corpus
## I dont want to remove symbols as they are often quite important in tweets
toks <- tokens(corpus, remove_punct = TRUE, remove_url=TRUE, remove_numbers = TRUE, verbose=TRUE)
## making 3 different dfms and using cross validation to decide which to use
toks_stop <- tokens_remove(toks, c(
  stopwords("english"),"https", "rt", "http", "u", "amp"))

toks_ngram <- tokens_ngrams(toks_stop, n = 1:2)
bigram_2_5000 <- dfm(toks_ngram, tolower=TRUE)
bigram_2_5000 <- dfm_trim(bigram_2_5000, min_docfreq = 2)

bigram_2_5000
# 10,491 features

toks_ngram <- tokens_ngrams(toks_stop, n = 1:3)
trigram_2_5000 <- dfm(toks_ngram, tolower=TRUE)
trigram_2_5000 <- dfm_trim(trigram_2_5000, min_docfreq = 2)

trigram_2_5000
# 12,250 features
```


# 10 fold CV function Naive Bayes
```{r}
library(quanteda.textmodels)
library(caret)
# 1st we iterate through each combination of parameters
ten_fold <- function(data){
  # randomly sorting the data.
  random_order <- data[sample(1:nrow(data)),]
  
  # taking the length of the dataframe
  n <- nrow(random_order)

  # 10 folds for CV error.
  k <- n/10
      
  F1_table <- data.frame(matrix(nrow = 10, ncol = 3))
  g = 0
  # the loop which performs 10 fold CV manually
  for (a in seq(0,n-k,k)) {
    g = g + 1
    # splits into train and test
    test_set <- random_order[(a+1):(a+k),]
    train_set <- random_order[-((a+1):(a+k)),]
    
    # training Naive Bayes model
    nb <- textmodel_nb(train_set, docvars(test_set, "class"))
    # predicting labels for test set
    preds <- predict(nb, newdata = test_set)
    #confusion matrix
    conf_matrix <- confusionMatrix(preds, docvars(test_set, "class"), mode = "everything")
    con <- conf_matrix$byClass
      
    fear <- con[2,7]
    fear <- ifelse(fear == "NaN", 0, fear)
    denial <- con[3,7]
    denial <- ifelse(denial == "NaN", 0, denial)
    
    F1_table[g,1] <- con[1,7]
    F1_table[g,2] <- fear
    F1_table[g,3] <- denial
    
    
  }
  results <- data.frame(c(mean(F1_table[,1])), c(mean(F1_table[,2])), c(mean(F1_table[,3])))
  colnames(results) <- c("F1 Neutral", "F1 Fear", "F1 Denial")
  
  return(results)

}

ten_fold(bigram_2_2000)
ten_fold(bigram_2_4000)
ten_fold(bigram_2_5000)
ten_fold(trigram_2_2000)
ten_fold(trigram_2_4000)
ten_fold(trigram_2_5000)

# the 2000 fails to identify any denial
```

## Ridge - Regularised Regression
This runs a cv with cv.glmnet to find the best lambda, and then runs a 10 fold cv to find the f1 for each digit
```{r}
set.seed(1)
library(glmnet)
library(quanteda.textmodels)
library(caret)

ten_fold_ridge <- function(data){
  # randomly sorting the data.
  random_order <- data[sample(1:nrow(data)),]
  
  # cross validation for lambda
  cv <- cv.glmnet(x = random_order,
                   y = docvars(random_order, "class"),
                   alpha = 0,
                   nfold = 5,
                   family = "multinomial",
                   type.measure = 'class')



  min <- cv$lambda.min
  
  # taking the length of the dataframe
  n <- nrow(random_order)

  # 10 folds for CV error.
  k <- n/10
      
  F1_table <- data.frame(matrix(nrow = 10, ncol = 3))
  g = 0
  # the loop which performs 10 fold CV manually
  for (a in seq(0,n-k,k)) {
    g = g + 1
    # splits into train and test
    test_set <- random_order[(a+1):(a+k),]
    train_set <- random_order[-((a+1):(a+k)),]
    
    # training Naive Bayes model
    lasso <- glmnet(x = train_set,
                   y =docvars(train_set, "class"),
                   alpha = 0,
                   family = "multinomial",
                   lambda = min)
    
    
    # predicting labels for test set
    preds <- predict(lasso, test_set, type = "class")
    
    preds <- preds[,1]
    preds <- as.factor(preds)
    
    #confusion matrix
    conf_matrix <- confusionMatrix(preds, docvars(test_set, "class"), mode = "everything")
    con <- conf_matrix$byClass
      
    fear <- con[2,7]
    fear <- ifelse(is.na(fear) == TRUE, 0, fear)
    denial <- con[3,7]
    denial <- ifelse(is.na(denial) == TRUE, 0, denial)
    
    F1_table[g,1] <- con[1,7]
    F1_table[g,2] <- fear
    F1_table[g,3] <- denial
    
    
  }
  results <- data.frame(c(mean(F1_table[,1])), c(mean(F1_table[,2])), c(mean(F1_table[,3])))
  colnames(results) <- c("F1 Neutral", "F1 Fear", "F1 Denial")
  
  return(results)

}

ten_fold_ridge(bigram_2_2000)
ten_fold_ridge(bigram_2_4000)
ten_fold_ridge(bigram_2_5000)
ten_fold_ridge(trigram_2_2000)
ten_fold_ridge(trigram_2_4000)
ten_fold_ridge(trigram_2_5000)

# the 4,000 dataset is the best performing,
# going to drop both 2,000 ones now as they are performing very poorly
```

## LASSO - Regularised Regression
```{r}
set.seed(1)
library(glmnet)
library(quanteda.textmodels)
library(caret)

ten_fold_lasso <- function(data){
  # randomly sorting the data.
  random_order <- data[sample(1:nrow(data)),]
  
  # cross validation for lambda
  cv <- cv.glmnet(x = random_order,
                   y = docvars(random_order, "class"),
                   alpha = 1,
                   nfold = 5,
                   family = "multinomial",
                   type.measure = 'class')



  min <- cv$lambda.min
  
  # taking the length of the dataframe
  n <- nrow(random_order)

  # 10 folds for CV error.
  k <- n/10
      
  F1_table <- data.frame(matrix(nrow = 10, ncol = 3))
  g = 0
  # the loop which performs 10 fold CV manually
  for (a in seq(0,n-k,k)) {
    g = g + 1
    # splits into train and test
    test_set <- random_order[(a+1):(a+k),]
    train_set <- random_order[-((a+1):(a+k)),]
    
    # training Naive Bayes model
    lasso <- glmnet(x = train_set,
                   y =docvars(train_set, "class"),
                   alpha = 1,
                   family = "multinomial",
                   lambda = min)
    
    
    # predicting labels for test set
    preds <- predict(lasso, test_set, type = "class")
    
    preds <- preds[,1]
    preds <- as.factor(preds)
    
    #confusion matrix
    conf_matrix <- confusionMatrix(preds, docvars(test_set, "class"), mode = "everything")
    con <- conf_matrix$byClass
    
    fear <- con[2,7]
    fear <- ifelse(is.na(fear) == TRUE, 0, fear)
    denial <- con[3,7]
    denial <- ifelse(is.na(denial) == TRUE, 0, denial)
    
    F1_table[g,1] <- con[1,7]
    F1_table[g,2] <- fear
    F1_table[g,3] <- denial
    
    
  }
  results <- data.frame(c(mean(F1_table[,1])), c(mean(F1_table[,2])), c(mean(F1_table[,3])))
  colnames(results) <- c("F1 Neutral", "F1 Fear", "F1 Denial")
  
  return(results)

}

ten_fold_lasso(bigram_2_4000)
ten_fold_lasso(bigram_2_5000)
ten_fold_lasso(trigram_2_4000)
ten_fold_lasso(trigram_2_5000)

# LASSO did not perform well on any of the sets
```

# SVM
```{r}
set.seed(1)

ten_fold_SVM_randomised <- function(data){
  
  # randomly sorting the data.
  random_order <- data[sample(1:nrow(data)),]
  
  # taking the length of the dataframe
  n <- nrow(random_order)

  # 10 folds for CV error.
  k <- n/10

  # Create a vector of reasonable values of C
  # Could not do random selection for a linear series between values in R as it would tend towards values greater than 1.
  C1 <- seq(0.00001, 0.001, 0.0001)
  C2 <- seq(0.002, 0.1, 0.005)
  C3 <- seq(0.2, 1, 0.1)
  C4 <- seq(2, 20, 2)
  C5 <- seq(25, 100, 25)
  C6 <- seq(100, 1000, 100)
  C <- c(C1, C2, C3, C4, C5, C6)
  
  # creating results table
  table <- data.frame(matrix(nrow = 100, ncol = 4))
  colnames(table) <- c("c value", "F1 Neutral", "F1 Fear", "F1 Denial")
  for (j in 1:20){
  # randomly sample for each of the 100 iterations from our hyperparameters
    c <- sample(C,replace = TRUE, size = 1)
    
    #storing cross validation errors
    F1_table <- data.frame(matrix(nrow = 10, ncol = 3))
    g = 0
    
    # the loop which performs 10 fold CV manually
    for (a in seq(0,n-k,k)) {
      g = g + 1
      # splits into train and test
      test_set <- random_order[(a+1):(a+k),]
      train_set <- random_order[-((a+1):(a+k)),]
    
      svmfit <- svm(x=train_set, y=docvars(train_set, "class"), kernel = "linear", cost = c, scale = FALSE)
    
      
      # predicting labels for test set
      preds <- predict(svmfit, newdata = test_set)
      
      #confusion matrix
      conf_matrix <- confusionMatrix(preds, docvars(test_set, "class"), mode = "everything")
      con<- conf_matrix$byClass
      
        
      fear <- con[2,7]
      fear <- ifelse(is.na(fear) == TRUE, 0, fear)
      denial <- con[3,7]
      denial <- ifelse(is.na(denial) == TRUE, 0, denial)
      
      F1_table[g,1] <- con[1,7]
      F1_table[g,2] <- fear
      F1_table[g,3] <- denial
    
    }
    # calculating the average scores
    table[j,1] <- c
    table[j,2] <- mean(F1_table[,1]) #F1 neutral
    table[j,3] <- mean(F1_table[,2]) #F1 fear
    table[j,4] <- mean(F1_table[,3]) #F1 Denial
    }


  return(table)
  
} 
              
bi_4000 <- ten_fold_SVM_randomised(bigram_2_4000)
bi_5000 <- ten_fold_SVM_randomised(bigram_2_5000)
tri_4000 <- ten_fold_SVM_randomised(trigram_2_4000)
tri_5000 <- ten_fold_SVM_randomised(trigram_2_5000)

bi_4000 %>%
  arrange(`F1 Fear`, decreasing = TRUE)

bi_5000 %>%
  arrange(`F1 Fear`, decreasing = TRUE)

tri_4000 %>%
  arrange(`F1 Fear`, decreasing = TRUE)

tri_5000 %>%
  arrange(`F1 Fear`, decreasing = TRUE)

```

# Random Forest
```{r}
library("randomForest")
library("pROC")
library(caret)
# 1st we iterate through each combination of parameters
ten_fold_rf <- function(data){
  data <- data %>%
    convert(to = "data.frame") %>%
    cbind(data$class)

  data <- rename(data, "label" = "data$class")
    
  # randomly sorting the data.
  random_order <- data[sample(1:nrow(data)),]
  
  # taking the length of the dataframe
  n <- nrow(random_order)

  # 10 folds for CV error.
  k <- n/10
      
  F1_table<- data.frame(matrix(nrow = 10, ncol = 3))
  g = 0
  # the loop which performs 10 fold CV manually
  for (a in seq(0,n-k,k)) {
    g = g + 1
    # splits into train and test
    test_set <- random_order[(a+1):(a+k),]
    train_set <- random_order[-((a+1):(a+k)),]
    
    # Estimation
    model_rf = randomForest(label~text, data=train_set, ntree = 500, importance=TRUE, trace.it = TRUE)
    
    # Prediction
    preds = predict(model_rf, newdata = test_set)
  
    #confusion matrix
    conf_matrix <- confusionMatrix(preds, test_set$label, mode = "everything")
    con <- conf_matrix$byClass
    
    fear <- con[2,7]
    fear <- ifelse(is.na(fear) == TRUE, 0, fear)
    denial <- con[3,7]
    denial <- ifelse(is.na(denial) == TRUE, 0, denial)
    
    F1_table[g,1] <- con[1,7]
    F1_table[g,2] <- fear
    F1_table[g,3] <- denial
    
    
  }
  results <- data.frame(c(mean(F1_table[,1])), c(mean(F1_table[,2])), c(mean(F1_table[,3])))
  colnames(results) <- c("F1 Neutral", "F1 Fear", "F1 Denial")
  
  return(results)

  
}

ten_fold_rf(bigram_2_4000)
ten_fold_rf(bigram_2_5000)
ten_fold_rf(trigram_2_4000)
ten_fold_rf(trigram_2_5000)
```

# Light GBM
```{r}
library("lightgbm")
library("dplyr")
library("pROC")
library("caret")

#ns_min_3$class
ten_fold_paramsGBM <- function(data){
  # randomly sorting the data.
  random_order <- data[sample(1:nrow(data)),]
  random_order$class <- as.numeric(random_order$class)
  
  random_order$class <-  random_order$class - 1

  # taking the length of the dataframe
  n <- nrow(random_order)

  # 5 folds for CV error.
  k <- n/10

  # creating results table
  CV_results <- data.frame(matrix(nrow = 100, ncol = 9))
  colnames(CV_results) <- c("learning rate", "num_leaves", "max_depth", "feature_fraction", "bagging_fraction", "is_unbalanced", "F1 Neutral", "F1 Fear", "F1 Denial")

  # an index of which row of the results we are at in the for loop.
  p = 0
  
  
  # 1st we iterate through each combination of parameters
  for (j in 1:100){
    p = p + 1
    
    # set the parameters
    # reducing the range slightly as a result of previous hyperparameter searches
    lr <- exp(runif(1, min = -4.6, max = -0.7)) 
    leaf <- round(runif(1, min = 2, max = 200))
    depth <- round(runif(1, min = 1, max = 30))
    ff <- runif(1, min = 0.2, max = 0.85)
    bf <- runif(1, min = 0, max = 1)
    is_u <- sample(c(TRUE, FALSE), 1)
  
    # calculating the average scores
    CV_results[j,1] <- lr  
    CV_results[j,2] <- leaf
    CV_results[j,3] <- depth
    CV_results[j,4] <- ff
    CV_results[j,5] <- bf
    CV_results[j,6] <- is_u
    
    
    # an index of which fold in 5-fold CV we are at
    g = 0
  
    # empty vector to store each misclassification error for each fold.
    F1_table <- matrix(nrow = 10, ncol = 3)
  
    # the loop which performs 10 fold CV manually
    for (a in seq(0,n-k,k)) {
      g = g + 1
      # splits into train and test
      test_set <- random_order[(a+1):(a+k),]
      train_set <- random_order[-((a+1):(a+k)),]
    
      print(nrow(train_set))
      
      # Training dataset
      tr_dataset <- lgb.Dataset(data = train_set[1:1500,],
                                label = as.numeric(train_set$class[1:1500]),
                                params = list(verbose = -1))
      # Validation dataset
      validation_dataset <- lgb.Dataset.create.valid(dataset = tr_dataset,
                                               data = train_set[1501:nrow(train_set),],
                                               label = as.numeric(train_set$class[1501:nrow(train_set)]),
                                               params = list(verbose = -1))

    
      
      # run the model on 1 of the 5 folds
      params <- list(
      objective = "multiclass"
      , metric = "multi_error"
      , num_class = 3L
      , learning_rate = lr
      , max_depth = depth
      , num_leaves = leaf
      , feature_fraction = ff
      , bagging_fraction = bf
      , is_unbalance = is_u
      , early_stopping = 50)
    
      model <- lgb.train(
        params = params, 
        data = tr_dataset,
        nrounds = 10000, # note: needs to be larger for very small learning rates
        valids = list(training = tr_dataset, validation = validation_dataset),
        verbose = -1
      )
    
      # Test set accuracy
      y_prob <- predict(model, test_set, reshape = TRUE)

      class_pred <- c()
      for (i in 1:nrow(y_prob)){
        class_pred[i] <- which.max(y_prob[i,])
      }
      
      class_pred <- class_pred - 1
      class_pred <- as.factor(class_pred)
      test_set$class <- as.factor(test_set$class)
      
      #confusion matrix
      conf_matrix <- confusionMatrix(class_pred, test_set$class, mode = "everything")
      con <- conf_matrix$byClass
    
      fear <- con[2,7]
      fear <- ifelse(is.na(fear) == TRUE, 0, fear)
      denial <- con[3,7]
      denial <- ifelse(is.na(denial) == TRUE, 0, denial)
    
      F1_table[g,1] <- con[1,7]
      F1_table[g,2] <- fear
      F1_table[g,3] <- denial
    }
  
  # taking the average of the 10 folds errors
  CV_results[j,7] <- mean(F1_table[,1])
  CV_results[j,8] <- mean(F1_table[,2])
  CV_results[j,9] <- mean(F1_table[,3])
 
  }
  
  return(CV_results)
  
  
}
  
GBM_bi_4000 <- ten_fold_paramsGBM(bigram_2_4000)
GBM_bi_5000 <- ten_fold_paramsGBM(bigram_2_5000)
GBM_tri_4000 <- ten_fold_paramsGBM(trigram_2_4000)
GBM_tri_5000 <- ten_fold_paramsGBM(trigram_2_5000)

GBM_bi_4000 %>%
  arrange(`F1 Fear`, decreasing = TRUE)

GBM_bi_5000 %>%
  arrange(`F1 Fear`, decreasing = TRUE)

GBM_tri_4000 %>%
  arrange(`F1 Fear`, decreasing = TRUE)

GBM_tri_5000 %>%
  arrange(`F1 Fear`, decreasing = TRUE)
```

# LASSO feature selection
```{r}
set.seed(1)
library(glmnet)
library(quanteda.textmodels)
lasso <- cv.glmnet(x = ns_min_3,
                   y = docvars(ns_min_3, "class"),
                   alpha = 1,
                   nfold = 5,
                   family = "multinomial")

vars <- coef(lasso, s = "lambda.min")

inds1 <-which(vars$`0` !=0)
inds2 <-which(vars$`1` !=0)
inds3 <-which(vars$`2` !=0)


inds <- c(inds1, inds2, inds3)
inds <- sort(unique(inds))

# now subset the rows of the dataframe with inds
lassod_min_3 <- ns_min_3[,inds]

ten_fold <- function(data){
  # randomly sorting the data.
  random_order <- data[sample(1:nrow(data)),]
  
  # taking the length of the dataframe
  n <- nrow(random_order)

  # 10 folds for CV error.
  k <- n/10
      
  CVerrors <- data.frame(matrix(nrow = 10, ncol = 3))
  g = 0
  # the loop which performs 10 fold CV manually
  for (a in seq(0,n-k,k)) {
    g = g + 1
    # splits into train and test
    test_set <- random_order[(a+1):(a+k),]
    train_set <- random_order[-((a+1):(a+k)),]
    
    # training Naive Bayes model
    nb <- textmodel_nb(random_order[(a+1):(a+k),], docvars(random_order, "class")[(a+1):(a+k)])
    # predicting labels for test set
    preds <- predict(nb, newdata = random_order[(a+1):(a+k),])
    # computing the confusion matrix
    results <- table(preds, docvars(random_order, "class")[(a+1):(a+k)])
    
     # creating the table
    CVerror <- c()
    
    # calculating the precision, recall and F1 for each digit
    for (i in 1:3){
      precision <- results[i,i]/sum(results[i,])
      recall <- results[i,i]/sum(results[,i])
      CVerrors[g,i] <- (2*precision*recall)/(precision + recall)
    }
    
    
  }
  results <- data.frame(c(mean(CVerrors[,1])), c(mean(CVerrors[,2])), c(mean(CVerrors[,3])))
  colnames(results) <- c("F1 Neutral", "F1 Fear", "F1 Denial")
  
  return(results)

}

ten_fold(lassod_min_3)

```

```{r}

# extracting coefficients
best.lambda <- which(lasso$lambda==lasso$lambda.1se)
beta <- lasso$glmnet.fit$beta[best.lambda]

lasso$glmnet.fit
## identifying predictive features
df <- data.frame(coef = as.numeric(beta),
                ngram = names(beta), stringsAsFactors=F)


ten_fold <- function(data){
  # randomly sorting the data.
  random_order <- data[sample(1:nrow(data)),]
  
  # taking the length of the dataframe
  n <- nrow(random_order)

  # 10 folds for CV error.
  k <- round(n/10)
     
  CVerrors <- data.frame(matrix(nrow = 10, ncol = 3))
  g = 0
  # the loop which performs 10 fold CV manually
  for (a in seq(0,n-k,k)) {
    g = g + 1
    # splits into train and test
    test_set <- random_order[(a+1):(a+k),]
    train_set <- random_order[-((a+1):(a+k)),]
    
    # training Naive Bayes model
    mod <- glmnet(x = train_set,
                   y =  train_set$class,
                   alpha = 1,
                   nfold = 5,
                   family = "multinomial",
                  lambda = 0.013185)
    
    # predicting labels for test set
    preds <- stats::predict(mod, test_set, type="class", s="lambda.1se")
    
    # computing the confusion matrix
    results <- table(preds, docvars(test_set, "class"))
    
    print(results)
     # creating the table
    CVerror <- c()
    
    # calculating the precision, recall and F1 for each digit
    for (i in 1:3){
      precision <- results[i,i]/sum(results[i,])
      recall <- results[i,i]/sum(results[,i])
      CVerrors[g,i] <- (2*precision*recall)/(precision + recall)
    }
    
    
    
  }
  results <- data.frame(c(mean(CVerrors[,1])), c(mean(CVerrors[,2])), c(mean(CVerrors[,3])))
  colnames(results) <- c("F1 Neutral", "F1 Fear", "F1 Denial")
  
  return(results)


  
}

ten_fold(ns_min_3)
```
