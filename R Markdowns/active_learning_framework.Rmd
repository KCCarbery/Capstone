---
title: "prediction test"
output: html_document
date: '2022-06-16'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
library(caret)
options(scipen = "100")
nz_data <- read_csv("../country_data/new_zealand.csv")
nz_data$Country <- "NZ"
training_set <- read_csv("../training_sets/train_4000.csv")
```
#### Merge
```{r}
library(dplyr)
options(scipen = "100")
nz_data <- nz_data %>% filter(lang.y == "en")
sample_merge <- merge(nz_data, training_set, all = TRUE, by.x = "text", by.y = "text")

sample_merge <- sample_merge %>%
  select(c(text, class, date_time, Country))
head(sample_merge)

nrow(sample_merge %>% filter(is.na(class)==TRUE))
nrow(sample_merge %>% filter(is.na(class)==FALSE))
```

## One Mega Function

```{r}
library(quanteda)
library(e1071)

active_learning <- function(data, min_freq, C, ixes, vals_1, vals_2, vals_3){
  
  # updating labels from the previous iterations
  if (length(vals_1) == 0){
    
  } else {
    ix_1 <- ixes[[1]]
    ix_2 <- ixes[[2]]
    ix_3 <- ixes[[3]]
    
    for (i in 1:length(ix_1)){
      data$class[ix_1[i]] <- vals_1[i]
      data$class[ix_2[i]] <- vals_2[i]
      data$class[ix_3[i]] <- vals_3[i]
      }
  }
  
  # creating a variable with the index
  data$index <- 1:nrow(data)
  
  # transform response variable to factor
  data$class <- factor(data$class)
  
  corpus <- corpus(data, text_field="text") 
  toks <- tokens(corpus, remove_punct = TRUE, remove_url=TRUE, remove_numbers = TRUE, verbose=TRUE)
  toks_stop <- tokens_remove(toks, c(stopwords("english"),"https", "rt", "http", "u", "amp"))
  toks_ngram <- tokens_ngrams(toks_stop, n = 1:2)
  mydfm <- dfm(toks_ngram, tolower=TRUE)
  mydfm <- dfm_trim(mydfm, min_docfreq = min_freq)

  # Separate labeled documents from unlabeled documents 
  unlabeled <- dfm_subset(mydfm, is.na(mydfm$class))
  labeled <- dfm_subset(mydfm, !is.na(mydfm$class))
  
  #checking its working
  print(nrow(unlabeled))
  
  svmfit <- svm(x=labeled, y=docvars(labeled, "class"), kernel = "linear", cost = C, scale = FALSE)
  
  # target observations closest to decision boundary
  pred <- predict(svmfit, unlabeled, decision.values = TRUE)
  
  # getting for the 3 different classes
  dist_1 <- abs(attr(pred, "decision.values")[,1])
  dist_2 <- abs(attr(pred, "decision.values")[,2])
  dist_3 <- abs(attr(pred, "decision.values")[,3])
  
  sorted_1 <- sort(dist_1, index.return = TRUE)
  sorted_2 <- sort(dist_2, index.return = TRUE)
  sorted_3 <- sort(dist_3, index.return = TRUE)
  
  index_1 <- sorted_1$ix[1:10]
  index_2 <- sorted_2$ix[1:10]
  index_3 <- sorted_3$ix[1:10]

  unlabeled_text <- sample_merge %>%
    filter(is.na(class)==TRUE)
  
  ix_1 <- unlabeled_text$index[index_1]
  ix_2 <- unlabeled_text$index[index_2]
  ix_3 <- unlabeled_text$index[index_3]
  
  texts <- list(unlabeled_text$text[index_1],unlabeled_text$text[index_2],unlabeled_text$text[index_3]) 
  indexes <- list(ix_1, ix_2, ix_3)
  
  return(list(texts, indexes, data))
}
go_1 <- active_learning(sample_merge, 3, 12, list(),c(),c(),c())
# 53686 length

ixes <- go_1[[2]]
data <- go_1[[3]]

## run a k-fold CV and report F1s
ten_fold_SVM <- function(data, min_freq, C){
  # transform response variable to factor
  data$class <- factor(data$class)
  
  corpus <- corpus(data, text_field="text") 
  toks <- tokens(corpus, remove_punct = TRUE, remove_url=TRUE, remove_numbers = TRUE, verbose=TRUE)
  toks_stop <- tokens_remove(toks, c(stopwords("english"),"https", "rt", "http", "u", "amp"))
  toks_ngram <- tokens_ngrams(toks_stop, n = 1:2)
  mydfm <- dfm(toks_ngram, tolower=TRUE)
  mydfm <- dfm_trim(mydfm, min_docfreq = min_freq)

  # Separate labeled documents from unlabeled documents 
  labeled <- dfm_subset(mydfm, !is.na(mydfm$class))
  
  # randomly sorting the data.
  random_order <- labeled[sample(1:nrow(labeled)),]
  
  # taking the length of the dataframe
  n <- nrow(random_order)

  # 10 folds for CV error.
  k <- n/10
  g = 0
  
  F1_table <- matrix(nrow = 10, ncol = 3)
  # the loop which performs 10 fold CV manually
  for (a in seq(0,n-k,k)) {
    g = g + 1
    # splits into train and test
    test_set <- random_order[(a+1):(a+k),]
    train_set <- random_order[-((a+1):(a+k)),]
    
    svmfit <- svm(x=train_set, y=docvars(train_set, "class"), kernel = "linear", cost = C, scale = FALSE)
    
    # predicting labels for test set
    preds <- predict(svmfit, newdata = test_set)
      
    #confusion matrix
    conf_matrix <- confusionMatrix(preds, docvars(test_set, "class"), mode = "everything")
    con<- conf_matrix$byClass
      
        
    fear <- con[2,7]
    fear <- ifelse(is.na(fear) == TRUE, 0, fear)
    denial <- con[3,7]
    denial <- ifelse(is.na(denial) == TRUE, 0, denial)
      
    F1_table[g,1] <- con[1,7]
    F1_table[g,2] <- fear
    F1_table[g,3] <- denial
    }

  results <- data.frame(c(mean(F1_table[,1])), c(mean(F1_table[,2])), c(mean(F1_table[,3])))
  colnames(results) <- c("F1 Neutral", "F1 Fear", "F1 Denial")
  
  return(results)
} 

ten_fold_SVM(data, 3, 12)
```

```{r}
go_1[[1]][[1]]
vals_1 <- c(2,0,0,0,0,0,0,0,0,2)

go_1[[1]][[2]]
vals_2 <- c(0,0,0,0,1,0,0,0,2,1)

go_1[[1]][[3]]
vals_3 <- c(0,2,0,0,0,0,0,0,0,0)

go_2 <- active_learning(data, 3, 12, ixes, vals_1, vals_2, vals_3)
# 53656 length

ixes <- go_2[[2]]
data <- go_2[[3]]

ten_fold_SVM(data, 3, 12)
```

